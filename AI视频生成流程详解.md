# ðŸŽ¬ Pixelle-Video AIè§†é¢‘ç”Ÿæˆæµç¨‹è¯¦è§£

## ðŸ“‹ ç›®å½•
- [æ•´ä½“æž¶æž„](#æ•´ä½“æž¶æž„)
- [æ ¸å¿ƒæµç¨‹](#æ ¸å¿ƒæµç¨‹)
- [æŠ€æœ¯æ ˆè¯¦è§£](#æŠ€æœ¯æ ˆè¯¦è§£)
- [å·¥ä½œæµé…ç½®](#å·¥ä½œæµé…ç½®)
- [éƒ¨ç½²æ–¹æ¡ˆ](#éƒ¨ç½²æ–¹æ¡ˆ)
- [ä½¿ç”¨å»ºè®®](#ä½¿ç”¨å»ºè®®)

---

## ðŸ—ï¸ æ•´ä½“æž¶æž„

Pixelle-Video é‡‡ç”¨**æ¨¡å—åŒ–ç®¡é“è®¾è®¡**ï¼ŒåŸºäºŽ ComfyUI æž¶æž„ï¼Œæ”¯æŒçµæ´»çš„AIèƒ½åŠ›ç»„åˆã€‚

### æž¶æž„å›¾
```
ç”¨æˆ·è¾“å…¥ â†’ ç®¡é“å¤„ç† â†’ AIæœåŠ¡ â†’ åª’ä½“ç”Ÿæˆ â†’ è§†é¢‘è¾“å‡º
    â†“         â†“         â†“         â†“         â†“
  ä¸»é¢˜æ–‡æœ¬   çº¿æ€§ç®¡é“   LLMæœåŠ¡   ComfyUI   æœ€ç»ˆè§†é¢‘
            æ ‡å‡†ç®¡é“   TTSæœåŠ¡   RunningHub
            èµ„äº§ç®¡é“   åª’ä½“æœåŠ¡
```

### æ ¸å¿ƒç»„ä»¶
- **ç®¡é“ç³»ç»Ÿ** (`pipelines/`): æµç¨‹ç¼–æŽ’å’ŒçŠ¶æ€ç®¡ç†
- **æœåŠ¡å±‚** (`services/`): AIæœåŠ¡æŠ½è±¡å’Œå®žçŽ°
- **å·¥ä½œæµ** (`workflows/`): ComfyUIé…ç½®æ–‡ä»¶
- **æ¨¡æ¿ç³»ç»Ÿ** (`templates/`): HTMLè§†é¢‘æ¨¡æ¿
- **Webç•Œé¢** (`web/`): Streamlitç”¨æˆ·ç•Œé¢

---

## ðŸ”„ æ ¸å¿ƒæµç¨‹

### æµç¨‹æ¦‚è§ˆ
```
è¾“å…¥ä¸»é¢˜ â†’ æ–‡æ¡ˆç”Ÿæˆ â†’ é…å›¾è§„åˆ’ â†’ é€å¸§å¤„ç† â†’ è§†é¢‘åˆæˆ
```

### è¯¦ç»†æ­¥éª¤

#### 1ï¸âƒ£ çŽ¯å¢ƒå‡†å¤‡é˜¶æ®µ (Setup Environment)
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `setup_environment()`

**åŠŸèƒ½**:
- åˆ›å»ºç‹¬ç«‹ä»»åŠ¡ç›®å½•: `output/task_{timestamp}/`
- ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
- åˆå§‹åŒ–è¾“å‡ºè·¯å¾„

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def setup_environment(self, ctx: PipelineContext):
    """Step 1: Setup task directory and environment."""
    text = ctx.input_text
    mode = ctx.params.get("mode", "generate")
    
    logger.info(f"ðŸš€ Starting StandardPipeline in '{mode}' mode")
    logger.info(f"   Text length: {len(text)} chars")
    
    # åˆ›å»ºç‹¬ç«‹ä»»åŠ¡ç›®å½•
    task_dir, task_id = create_task_output_dir()
    ctx.task_id = task_id
    ctx.task_dir = task_dir
    
    logger.info(f"ðŸ“ Task directory created: {task_dir}")
    logger.info(f"   Task ID: {task_id}")
    
    # ç¡®å®šæœ€ç»ˆè§†é¢‘è·¯å¾„
    output_path = ctx.params.get("output_path")
    if output_path is None:
        ctx.final_video_path = get_task_final_video_path(task_id)
    else:
        ctx.final_video_path = get_task_final_video_path(task_id)
        logger.info(f"   Will copy final video to: {output_path}")
```

**å·¥å…·å‡½æ•°å®žçŽ°**:
```python
# pixelle_video/utils/os_util.py
def create_task_output_dir():
    """åˆ›å»ºä»»åŠ¡è¾“å‡ºç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    task_id = f"task_{timestamp}_{random.randint(1000, 9999)}"
    task_dir = Path("output") / task_id
    task_dir.mkdir(parents=True, exist_ok=True)
    return str(task_dir), task_id

def get_task_final_video_path(task_id: str) -> str:
    """èŽ·å–ä»»åŠ¡æœ€ç»ˆè§†é¢‘è·¯å¾„"""
    return f"output/{task_id}/final_video.mp4"

def get_task_frame_path(task_id: str, frame_index: int, file_type: str) -> str:
    """èŽ·å–å¸§æ–‡ä»¶è·¯å¾„"""
    extensions = {
        "audio": ".mp3",
        "image": ".png", 
        "video": ".mp4",
        "composed": ".png",
        "segment": ".mp4"
    }
    ext = extensions.get(file_type, ".tmp")
    return f"output/{task_id}/frame_{frame_index:03d}_{file_type}{ext}"
```

---

#### 2ï¸âƒ£ å†…å®¹ç”Ÿæˆé˜¶æ®µ (Generate Content)
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `generate_content()`

**ä¸¤ç§æ¨¡å¼**:

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def generate_content(self, ctx: PipelineContext):
    """Step 2: Generate or process script/narrations."""
    mode = ctx.params.get("mode", "generate")
    text = ctx.input_text
    n_scenes = ctx.params.get("n_scenes", 5)
    min_words = ctx.params.get("min_narration_words", 5)
    max_words = ctx.params.get("max_narration_words", 20)
    
    if mode == "generate":
        # AIç”Ÿæˆæ¨¡å¼: ä½¿ç”¨LLMæ ¹æ®ä¸»é¢˜ç”Ÿæˆåˆ†é•œæ–‡æ¡ˆ
        self._report_progress(ctx.progress_callback, "generating_narrations", 0.05)
        ctx.narrations = await generate_narrations_from_topic(
            self.llm,
            topic=text,
            n_scenes=n_scenes,
            min_words=min_words,
            max_words=max_words
        )
        logger.info(f"âœ… Generated {len(ctx.narrations)} narrations")
    else:  # fixed
        # å›ºå®šæ–‡æ¡ˆæ¨¡å¼: ä½¿ç”¨ç”¨æˆ·æä¾›çš„å®Œæ•´è„šæœ¬
        self._report_progress(ctx.progress_callback, "splitting_script", 0.05)
        split_mode = ctx.params.get("split_mode", "paragraph")
        ctx.narrations = await split_narration_script(text, split_mode=split_mode)
        logger.info(f"âœ… Split script into {len(ctx.narrations)} segments (mode={split_mode})")
```

**æ ¸å¿ƒå‡½æ•°å®žçŽ°**:
```python
# pixelle_video/utils/content_generators.py
async def generate_narrations_from_topic(
    llm_service,
    topic: str,
    n_scenes: int = 5,
    min_words: int = 5,
    max_words: int = 20
) -> List[str]:
    """æ ¹æ®ä¸»é¢˜ç”Ÿæˆåˆ†é•œæ–‡æ¡ˆ"""
    
    prompt = f"""
è¯·æ ¹æ®ä¸»é¢˜"{topic}"åˆ›ä½œä¸€ä¸ªçŸ­è§†é¢‘çš„åˆ†é•œè„šæœ¬ã€‚

è¦æ±‚:
1. æ€»å…±{n_scenes}ä¸ªåˆ†é•œ
2. æ¯ä¸ªåˆ†é•œ{min_words}-{max_words}ä¸ªå­—
3. å†…å®¹è¦æœ‰é€»è¾‘æ€§å’Œè¿žè´¯æ€§
4. é€‚åˆçŸ­è§†é¢‘ä¼ æ’­
5. æ¯è¡Œä¸€ä¸ªåˆ†é•œï¼Œä¸è¦ç¼–å·

ä¸»é¢˜: {topic}
"""
    
    response = await llm_service.generate(prompt)
    
    # è§£æžå“åº”ï¼ŒæŒ‰è¡Œåˆ†å‰²
    narrations = []
    for line in response.strip().split('\n'):
        line = line.strip()
        if line and not line.startswith('#'):
            # æ¸…ç†å¯èƒ½çš„ç¼–å·
            line = re.sub(r'^\d+[.\s]*', '', line)
            if len(line) >= min_words:
                narrations.append(line)
    
    return narrations[:n_scenes]

async def split_narration_script(
    script: str, 
    split_mode: str = "paragraph"
) -> List[str]:
    """åˆ†å‰²å›ºå®šè„šæœ¬"""
    
    if split_mode == "paragraph":
        # æŒ‰æ®µè½åˆ†å‰²
        narrations = [p.strip() for p in script.split('\n\n') if p.strip()]
    elif split_mode == "sentence":
        # æŒ‰å¥å­åˆ†å‰²
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', script)
        narrations = [s.strip() for s in sentences if s.strip()]
    else:  # line
        # æŒ‰è¡Œåˆ†å‰²
        narrations = [line.strip() for line in script.split('\n') if line.strip()]
    
    return narrations
```

---

#### 3ï¸âƒ£ æ ‡é¢˜ç¡®å®šé˜¶æ®µ (Determine Title)
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `determine_title()`

**ç­–ç•¥**:
- ç”¨æˆ·æŒ‡å®šæ ‡é¢˜: ç›´æŽ¥ä½¿ç”¨
- AIç”Ÿæˆæ¨¡å¼: è°ƒç”¨ `generate_title()` è‡ªåŠ¨ç”Ÿæˆ
- å›ºå®šæ–‡æ¡ˆæ¨¡å¼: LLMåŸºäºŽå†…å®¹ç”Ÿæˆæ ‡é¢˜

---

#### 4ï¸âƒ£ è§†è§‰è§„åˆ’é˜¶æ®µ (Plan Visuals)
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `plan_visuals()`

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def plan_visuals(self, ctx: PipelineContext):
    """Step 4: Generate image prompts or visual descriptions."""
    # æ£€æµ‹æ¨¡æ¿ç±»åž‹å†³å®šæ˜¯å¦éœ€è¦åª’ä½“ç”Ÿæˆ
    frame_template = ctx.params.get("frame_template") or "1080x1920/default.html"
    
    template_name = Path(frame_template).name
    template_type = get_template_type(template_name)
    template_requires_media = (template_type in ["image", "video"])
    
    if template_type == "image":
        logger.info(f"ðŸ“¸ Template requires image generation")
    elif template_type == "video":
        logger.info(f"ðŸŽ¬ Template requires video generation")
    else:  # static
        logger.info(f"âš¡ Static template - skipping media generation pipeline")
        logger.info(f"   ðŸ’¡ Benefits: Faster generation + Lower cost + No ComfyUI dependency")
    
    # åªæœ‰æ¨¡æ¿éœ€è¦åª’ä½“æ—¶æ‰ç”Ÿæˆå›¾åƒæç¤ºè¯
    if template_requires_media:
        self._report_progress(ctx.progress_callback, "generating_image_prompts", 0.15)
        
        prompt_prefix = ctx.params.get("prompt_prefix")
        min_words = ctx.params.get("min_image_prompt_words", 30)
        max_words = ctx.params.get("max_image_prompt_words", 60)
        
        # ä¸´æ—¶è¦†ç›–prompt_prefixé…ç½®
        original_prefix = None
        if prompt_prefix is not None:
            image_config = self.core.config.get("comfyui", {}).get("image", {})
            original_prefix = image_config.get("prompt_prefix")
            image_config["prompt_prefix"] = prompt_prefix
            logger.info(f"Using custom prompt_prefix: '{prompt_prefix}'")
        
        try:
            # åˆ›å»ºè¿›åº¦å›žè°ƒåŒ…è£…å™¨
            def image_prompt_progress(completed: int, total: int, message: str):
                batch_progress = completed / total if total > 0 else 0
                overall_progress = 0.15 + (batch_progress * 0.15)
                self._report_progress(
                    ctx.progress_callback,
                    "generating_image_prompts",
                    overall_progress,
                    extra_info=message
                )
            
            # ç”ŸæˆåŸºç¡€å›¾åƒæç¤ºè¯
            base_image_prompts = await generate_image_prompts(
                self.llm,
                narrations=ctx.narrations,
                min_words=min_words,
                max_words=max_words,
                progress_callback=image_prompt_progress
            )
            
            # åº”ç”¨æç¤ºè¯å‰ç¼€
            image_config = self.core.config.get("comfyui", {}).get("image", {})
            prompt_prefix_to_use = prompt_prefix if prompt_prefix is not None else image_config.get("prompt_prefix", "")
            
            ctx.image_prompts = []
            for base_prompt in base_image_prompts:
                final_prompt = build_image_prompt(base_prompt, prompt_prefix_to_use)
                ctx.image_prompts.append(final_prompt)
        
        finally:
            # æ¢å¤åŽŸå§‹prompt_prefix
            if original_prefix is not None:
                image_config["prompt_prefix"] = original_prefix
        
        logger.info(f"âœ… Generated {len(ctx.image_prompts)} image prompts")
    else:
        # é™æ€æ¨¡æ¿ - å®Œå…¨è·³è¿‡å›¾åƒæç¤ºè¯ç”Ÿæˆ
        ctx.image_prompts = [None] * len(ctx.narrations)
        logger.info(f"âš¡ Skipped image prompt generation (static template)")
        logger.info(f"   ðŸ’¡ Savings: {len(ctx.narrations)} LLM calls + {len(ctx.narrations)} media generations")
```

**å·¥å…·å‡½æ•°å®žçŽ°**:
```python
# pixelle_video/utils/template_util.py
def get_template_type(template_name: str) -> str:
    """æ ¹æ®æ¨¡æ¿åç§°ç¡®å®šç±»åž‹"""
    if template_name.startswith("static_"):
        return "static"
    elif template_name.startswith("image_"):
        return "image"
    elif template_name.startswith("video_"):
        return "video"
    else:
        # é»˜è®¤ä¸ºå›¾ç‰‡æ¨¡æ¿
        return "image"

# pixelle_video/utils/content_generators.py
async def generate_image_prompts(
    llm_service,
    narrations: List[str],
    min_words: int = 30,
    max_words: int = 60,
    progress_callback=None
) -> List[str]:
    """ä¸ºæ¯ä¸ªåˆ†é•œç”Ÿæˆå›¾åƒæç¤ºè¯"""
    
    image_prompts = []
    total = len(narrations)
    
    for i, narration in enumerate(narrations):
        if progress_callback:
            progress_callback(i, total, f"Generating prompt for scene {i+1}")
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹è§†é¢‘åˆ†é•œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å›¾åƒæè¿°æç¤ºè¯ï¼Œç”¨äºŽAIå›¾åƒç”Ÿæˆã€‚

åˆ†é•œå†…å®¹: {narration}

è¦æ±‚:
1. æè¿°è¦å…·ä½“ç”ŸåŠ¨ï¼Œ{min_words}-{max_words}ä¸ªè¯
2. åŒ…å«åœºæ™¯ã€äººç‰©ã€åŠ¨ä½œã€çŽ¯å¢ƒç­‰ç»†èŠ‚
3. é€‚åˆAIå›¾åƒç”Ÿæˆæ¨¡åž‹ç†è§£
4. ä½¿ç”¨è‹±æ–‡è¾“å‡º
5. ä¸è¦åŒ…å«æ–‡å­—ã€å­—å¹•ç­‰å…ƒç´ 

è¯·ç›´æŽ¥è¾“å‡ºå›¾åƒæè¿°ï¼Œä¸è¦å…¶ä»–å†…å®¹:
"""
        
        response = await llm_service.generate(prompt)
        image_prompt = response.strip()
        image_prompts.append(image_prompt)
    
    if progress_callback:
        progress_callback(total, total, "All prompts generated")
    
    return image_prompts

# pixelle_video/utils/prompt_helper.py
def build_image_prompt(base_prompt: str, prompt_prefix: str = "") -> str:
    """æž„å»ºæœ€ç»ˆçš„å›¾åƒæç¤ºè¯"""
    if not prompt_prefix:
        return base_prompt
    
    # ç»„åˆå‰ç¼€å’ŒåŸºç¡€æç¤ºè¯
    if prompt_prefix.endswith(",") or base_prompt.startswith(","):
        final_prompt = f"{prompt_prefix} {base_prompt}".strip()
    else:
        final_prompt = f"{prompt_prefix}, {base_prompt}"
    
    return final_prompt
```

---

#### 5ï¸âƒ£ æ•…äº‹æ¿åˆå§‹åŒ– (Initialize Storyboard)
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `initialize_storyboard()`

**é…ç½®å¯¹è±¡åˆ›å»º**:
```python
ctx.config = StoryboardConfig(
    task_id=ctx.task_id,
    n_storyboard=len(ctx.narrations),
    tts_inference_mode="local" | "comfyui",
    voice_id="zh-CN-YunjianNeural",
    tts_workflow="tts_edge.json",
    media_workflow="image_flux.json",
    frame_template="1080x1920/default.html"
)
```

**æ•…äº‹æ¿åˆ›å»º**:
- åˆ›å»º `Storyboard` å¯¹è±¡åŒ…å«æ ‡é¢˜å’Œé…ç½®
- ä¸ºæ¯ä¸ªåˆ†é•œåˆ›å»º `StoryboardFrame` å¯¹è±¡
- å…³è”æ–‡æ¡ˆå’Œå›¾åƒæç¤ºè¯

---

#### 6ï¸âƒ£ èµ„äº§ç”Ÿäº§é˜¶æ®µ (Produce Assets) â­ æ ¸å¿ƒ
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `produce_assets()`

è¿™æ˜¯æœ€é‡è¦çš„é˜¶æ®µï¼Œå¯¹æ¯ä¸€å¸§è¿›è¡Œå®Œæ•´å¤„ç†ã€‚

**å¹¶è¡Œå¤„ç†ä¼˜åŒ–**:
```python
# RunningHubå·¥ä½œæµæ”¯æŒå¹¶è¡Œå¤„ç†
if is_runninghub and runninghub_concurrent_limit > 1:
    semaphore = asyncio.Semaphore(runninghub_concurrent_limit)
    tasks = [process_frame_with_semaphore(i, frame) for i, frame in enumerate(frames)]
    results = await asyncio.gather(*tasks)
```

**å•å¸§å¤„ç†æµç¨‹** (`services/frame_processor.py`):

##### 6.1 ðŸŽ¤ TTSéŸ³é¢‘ç”Ÿæˆ
**æ–‡ä»¶**: `services/frame_processor.py` - `_step_generate_audio()`

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def _step_generate_audio(
    self,
    frame: StoryboardFrame,
    config: StoryboardConfig
):
    """Step 1: Generate audio using TTS"""
    logger.debug(f"  1/4: Generating audio for frame {frame.index}...")
    
    # ç”Ÿæˆè¾“å‡ºè·¯å¾„
    from pixelle_video.utils.os_util import get_task_frame_path
    output_path = get_task_frame_path(config.task_id, frame.index, "audio")
    
    # æ ¹æ®æŽ¨ç†æ¨¡å¼æž„å»ºTTSå‚æ•°
    tts_params = {
        "text": frame.narration,
        "inference_mode": config.tts_inference_mode,
        "output_path": output_path,
        "index": frame.index + 1,  # 1-based index for workflow
    }
    
    if config.tts_inference_mode == "local":
        # æœ¬åœ°æ¨¡å¼: ä¼ é€’è¯­éŸ³å’Œé€Ÿåº¦å‚æ•°
        if config.voice_id:
            tts_params["voice"] = config.voice_id
        if config.tts_speed is not None:
            tts_params["speed"] = config.tts_speed
    else:  # comfyui
        # ComfyUIæ¨¡å¼: ä¼ é€’å·¥ä½œæµã€è¯­éŸ³ã€é€Ÿåº¦å’Œå‚è€ƒéŸ³é¢‘
        if config.tts_workflow:
            tts_params["workflow"] = config.tts_workflow
        if config.voice_id:
            tts_params["voice"] = config.voice_id
        if config.tts_speed is not None:
            tts_params["speed"] = config.tts_speed
        if config.ref_audio:
            tts_params["ref_audio"] = config.ref_audio
    
    # è°ƒç”¨TTSæœåŠ¡
    audio_path = await self.core.tts(**tts_params)
    
    frame.audio_path = audio_path
    
    # èŽ·å–éŸ³é¢‘æ—¶é•¿
    frame.duration = await self._get_audio_duration(audio_path)
    
    logger.debug(f"  âœ“ Audio generated: {audio_path} ({frame.duration:.2f}s)")

async def _get_audio_duration(self, audio_path: str) -> float:
    """èŽ·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
    try:
        # ä½¿ç”¨ffmpeg-pythonèŽ·å–æ—¶é•¿
        import ffmpeg
        probe = ffmpeg.probe(audio_path)
        duration = float(probe['format']['duration'])
        return duration
    except Exception as e:
        logger.warning(f"Failed to get audio duration: {e}, using estimate")
        # å¤‡ç”¨æ–¹æ¡ˆ: æ ¹æ®æ–‡ä»¶å¤§å°ä¼°ç®—ï¼ˆéžå¸¸ç²—ç•¥ï¼‰
        import os
        file_size = os.path.getsize(audio_path)
        # å‡è®¾MP3çº¦16kbpsï¼Œå³æ¯ç§’2KB
        estimated_duration = file_size / 2000
        return max(1.0, estimated_duration)  # è‡³å°‘1ç§’
```

**TTSæœåŠ¡å®žçŽ°**:
```python
# pixelle_video/services/tts_service.py
class TTSService:
    async def __call__(
        self,
        text: str,
        inference_mode: str = "local",
        voice: str = "zh-CN-YunjianNeural",
        speed: float = 1.2,
        workflow: str = None,
        ref_audio: str = None,
        output_path: str = None,
        **kwargs
    ) -> str:
        """TTSéŸ³é¢‘ç”Ÿæˆç»Ÿä¸€æŽ¥å£"""
        
        if inference_mode == "local":
            return await self._generate_local_tts(
                text=text,
                voice=voice,
                speed=speed,
                output_path=output_path
            )
        else:  # comfyui
            return await self._generate_comfyui_tts(
                text=text,
                workflow=workflow,
                voice=voice,
                speed=speed,
                ref_audio=ref_audio,
                output_path=output_path,
                **kwargs
            )
    
    async def _generate_local_tts(
        self,
        text: str,
        voice: str,
        speed: float,
        output_path: str
    ) -> str:
        """æœ¬åœ°Edge-TTSç”Ÿæˆ"""
        import edge_tts
        
        # è°ƒæ•´è¯­é€Ÿæ ¼å¼
        rate = f"{int((speed - 1) * 100):+d}%"
        
        communicate = edge_tts.Communicate(text, voice, rate=rate)
        await communicate.save(output_path)
        
        return output_path
    
    async def _generate_comfyui_tts(
        self,
        text: str,
        workflow: str,
        voice: str = None,
        speed: float = None,
        ref_audio: str = None,
        output_path: str = None,
        **kwargs
    ) -> str:
        """ComfyUIå·¥ä½œæµTTSç”Ÿæˆ"""
        from pixelle_video.services.comfy_base_service import ComfyBaseService
        
        # æž„å»ºå·¥ä½œæµå‚æ•°
        workflow_params = {
            "text": text,
        }
        
        if voice:
            workflow_params["voice"] = voice
        if speed:
            workflow_params["speed"] = speed
        if ref_audio:
            workflow_params["ref_audio"] = ref_audio
        
        # æ‰§è¡ŒComfyUIå·¥ä½œæµ
        comfy_service = ComfyBaseService(self.core.config)
        result = await comfy_service.execute_workflow(
            workflow_path=workflow,
            params=workflow_params,
            **kwargs
        )
        
        # ä¸‹è½½éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°
        if result.audio_url:
            await self._download_file(result.audio_url, output_path)
            return output_path
        else:
            raise ValueError("No audio generated from ComfyUI workflow")
```

##### 6.2 ðŸŽ¨ åª’ä½“ç”Ÿæˆ
**æ–‡ä»¶**: `services/frame_processor.py` - `_step_generate_media()`

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def _step_generate_media(
    self,
    frame: StoryboardFrame,
    config: StoryboardConfig
):
    """Step 2: Generate media (image or video) using ComfyKit"""
    logger.debug(f"  2/4: Generating media for frame {frame.index}...")
    
    # æ ¹æ®å·¥ä½œæµç¡®å®šåª’ä½“ç±»åž‹
    workflow_name = config.media_workflow or ""
    is_video_workflow = "video_" in workflow_name.lower()
    media_type = "video" if is_video_workflow else "image"
    
    logger.debug(f"  â†’ Media type: {media_type} (workflow: {workflow_name})")
    
    # æž„å»ºåª’ä½“ç”Ÿæˆå‚æ•°
    media_params = {
        "prompt": frame.image_prompt,
        "workflow": config.media_workflow,  # ä»Žé…ç½®ä¼ é€’å·¥ä½œæµ
        "media_type": media_type,
        "width": config.media_width,
        "height": config.media_height,
        "index": frame.index + 1,  # å·¥ä½œæµä½¿ç”¨1åŸºç´¢å¼•
    }
    
    # è§†é¢‘å·¥ä½œæµ: ä¼ é€’éŸ³é¢‘æ—¶é•¿ä½œä¸ºç›®æ ‡è§†é¢‘æ—¶é•¿
    # è¿™ç¡®ä¿è§†é¢‘é•¿åº¦ä¸ŽéŸ³é¢‘é•¿åº¦åŒ¹é…
    if is_video_workflow and frame.duration:
        media_params["duration"] = frame.duration
        logger.info(f"  â†’ Generating video with target duration: {frame.duration:.2f}s (from TTS audio)")
    
    # è°ƒç”¨åª’ä½“ç”ŸæˆæœåŠ¡
    media_result = await self.core.media(**media_params)
    
    # å­˜å‚¨åª’ä½“ç±»åž‹
    frame.media_type = media_result.media_type
    
    if media_result.is_image:
        # ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
        local_path = await self._download_media(
            media_result.url,
            frame.index,
            config.task_id,
            media_type="image"
        )
        frame.image_path = local_path
        logger.debug(f"  âœ“ Image generated: {local_path}")
    
    elif media_result.is_video:
        # ä¸‹è½½è§†é¢‘åˆ°æœ¬åœ°
        local_path = await self._download_media(
            media_result.url,
            frame.index,
            config.task_id,
            media_type="video"
        )
        frame.video_path = local_path
        
        # ä»Žè§†é¢‘ç»“æžœæ›´æ–°æ—¶é•¿
        if media_result.duration:
            frame.duration = media_result.duration
            logger.debug(f"  âœ“ Video generated: {local_path} (duration: {frame.duration:.2f}s)")
        else:
            # ä»Žæ–‡ä»¶èŽ·å–è§†é¢‘æ—¶é•¿
            frame.duration = await self._get_video_duration(local_path)
            logger.debug(f"  âœ“ Video generated: {local_path} (duration: {frame.duration:.2f}s)")
    
    else:
        raise ValueError(f"Unknown media type: {media_result.media_type}")

async def _download_media(
    self,
    url: str,
    frame_index: int,
    task_id: str,
    media_type: str
) -> str:
    """ä»ŽURLä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°"""
    from pixelle_video.utils.os_util import get_task_frame_path
    output_path = get_task_frame_path(task_id, frame_index, media_type)
    
    timeout = httpx.Timeout(connect=10.0, read=60, write=60, pool=60)
    async with httpx.AsyncClient(timeout=timeout) as client:
        response = await client.get(url)
        response.raise_for_status()
        
        with open(output_path, 'wb') as f:
            f.write(response.content)
    
    return output_path

async def _get_video_duration(self, video_path: str) -> float:
    """èŽ·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
    try:
        import ffmpeg
        probe = ffmpeg.probe(video_path)
        duration = float(probe['format']['duration'])
        return duration
    except Exception as e:
        logger.warning(f"Failed to get video duration: {e}, using audio duration")
        return 1.0  # é»˜è®¤1ç§’
```

**åª’ä½“æœåŠ¡å®žçŽ°**:
```python
# pixelle_video/services/media.py
class MediaService:
    async def __call__(
        self,
        prompt: str,
        workflow: str = None,
        media_type: str = "image",
        width: int = 1024,
        height: int = 1024,
        duration: float = None,
        **kwargs
    ) -> MediaResult:
        """åª’ä½“ç”Ÿæˆç»Ÿä¸€æŽ¥å£"""
        
        # ç¡®å®šå·¥ä½œæµè·¯å¾„
        if not workflow:
            workflow = "image_flux.json" if media_type == "image" else "video_wan2.1.json"
        
        # æž„å»ºå·¥ä½œæµå‚æ•°
        workflow_params = {
            "prompt": prompt,
            "width": width,
            "height": height,
        }
        
        # è§†é¢‘ç‰¹å®šå‚æ•°
        if media_type == "video" and duration:
            workflow_params["duration"] = duration
        
        # æ‰§è¡ŒComfyUIå·¥ä½œæµ
        from pixelle_video.services.comfy_base_service import ComfyBaseService
        comfy_service = ComfyBaseService(self.core.config)
        
        result = await comfy_service.execute_workflow(
            workflow_path=workflow,
            params=workflow_params,
            **kwargs
        )
        
        return MediaResult(
            media_type=media_type,
            url=result.output_url,
            duration=result.duration if media_type == "video" else None,
            width=width,
            height=height
        )

@dataclass
class MediaResult:
    """åª’ä½“ç”Ÿæˆç»“æžœ"""
    media_type: str  # "image" or "video"
    url: str
    duration: Optional[float] = None
    width: int = 1024
    height: int = 1024
    
    @property
    def is_image(self) -> bool:
        return self.media_type == "image"
    
    @property
    def is_video(self) -> bool:
        return self.media_type == "video"
```

##### 6.3 ðŸ–¼ï¸ å¸§åˆæˆ
**æ–‡ä»¶**: `services/frame_processor.py` - `_step_compose_frame()`

**HTMLæ¨¡æ¿æ¸²æŸ“**:
- ä½¿ç”¨ `HTMLFrameGenerator` æ¸²æŸ“æœ€ç»ˆç”»é¢
- æ·»åŠ å­—å¹•ã€æ ‡é¢˜ç­‰æ–‡å­—å…ƒç´ 
- æ”¯æŒå¤šç§å°ºå¯¸ï¼ˆç«–å±1080x1920/æ¨ªå±1920x1080/æ–¹å½¢1080x1080ï¼‰

**æ¨¡æ¿ç³»ç»Ÿ**:
```python
generator = HTMLFrameGenerator(template_path)
composed_path = await generator.generate_frame(
    title=storyboard.title,
    text=frame.narration,
    image=media_path,  # æ”¯æŒå›¾ç‰‡å’Œè§†é¢‘
    ext=custom_params
)
```

##### 6.4 ðŸŽ¬ è§†é¢‘ç‰‡æ®µåˆ›å»º
**æ–‡ä»¶**: `services/frame_processor.py` - `_step_create_video_segment()`

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def _step_create_video_segment(
    self,
    frame: StoryboardFrame,
    config: StoryboardConfig
):
    """Step 4: Create video segment from media + audio"""
    logger.debug(f"  4/4: Creating video segment for frame {frame.index}...")
    
    # ç”Ÿæˆè¾“å‡ºè·¯å¾„
    from pixelle_video.utils.os_util import get_task_frame_path
    output_path = get_task_frame_path(config.task_id, frame.index, "segment")
    
    from pixelle_video.services.video import VideoService
    video_service = VideoService()
    
    # æ ¹æ®åª’ä½“ç±»åž‹åˆ†æ”¯å¤„ç†
    if frame.media_type == "video":
        # è§†é¢‘å·¥ä½œæµ: åœ¨è§†é¢‘ä¸Šå åŠ HTMLæ¨¡æ¿ï¼Œç„¶åŽæ·»åŠ éŸ³é¢‘
        logger.debug(f"  â†’ Using video-based composition with HTML overlay")
        
        # æ­¥éª¤1: åœ¨è§†é¢‘ä¸Šå åŠ é€æ˜ŽHTMLå›¾åƒ
        # composed_image_pathåŒ…å«å¸¦é€æ˜ŽèƒŒæ™¯çš„æ¸²æŸ“HTML
        temp_video_with_overlay = get_task_frame_path(config.task_id, frame.index, "video") + "_overlay.mp4"
        
        video_service.overlay_image_on_video(
            video=frame.video_path,
            overlay_image=frame.composed_image_path,
            output=temp_video_with_overlay,
            scale_mode="contain"  # ç¼©æ”¾è§†é¢‘ä»¥é€‚åº”æ¨¡æ¿å°ºå¯¸
        )
        
        # æ­¥éª¤2: ä¸ºå åŠ åŽçš„è§†é¢‘æ·»åŠ æ—ç™½éŸ³é¢‘
        # æ³¨æ„: è§†é¢‘å¯èƒ½æœ‰éŸ³é¢‘ï¼ˆè¢«æ›¿æ¢ï¼‰æˆ–é™éŸ³ï¼ˆæ·»åŠ éŸ³é¢‘ï¼‰
        segment_path = video_service.merge_audio_video(
            video=temp_video_with_overlay,
            audio=frame.audio_path,
            output=output_path,
            replace_audio=True,  # ç”¨æ—ç™½æ›¿æ¢è§†é¢‘éŸ³é¢‘
            audio_volume=1.0
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import os
        if os.path.exists(temp_video_with_overlay):
            os.unlink(temp_video_with_overlay)
    
    elif frame.media_type == "image" or frame.media_type is None:
        # å›¾åƒå·¥ä½œæµ: ç›´æŽ¥ä½¿ç”¨åˆæˆå›¾åƒ
        # asset_default.htmlæ¨¡æ¿åœ¨åˆæˆä¸­åŒ…å«å›¾åƒ
        logger.debug(f"  â†’ Using image-based composition")
        
        segment_path = video_service.create_video_from_image(
            image=frame.composed_image_path,
            audio=frame.audio_path,
            output=output_path,
            fps=config.video_fps
        )
    
    else:
        raise ValueError(f"Unknown media type: {frame.media_type}")
    
    frame.video_segment_path = segment_path
    
    logger.debug(f"  âœ“ Video segment created: {segment_path}")
```

**è§†é¢‘æœåŠ¡å®žçŽ°**:
```python
# pixelle_video/services/video.py
class VideoService:
    def overlay_image_on_video(
        self,
        video: str,
        overlay_image: str,
        output: str,
        scale_mode: str = "contain"
    ):
        """åœ¨è§†é¢‘ä¸Šå åŠ å›¾åƒ"""
        import ffmpeg
        
        # èŽ·å–è§†é¢‘ä¿¡æ¯
        probe = ffmpeg.probe(video)
        video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')
        video_width = int(video_info['width'])
        video_height = int(video_info['height'])
        
        # æž„å»ºffmpegå‘½ä»¤
        video_input = ffmpeg.input(video)
        overlay_input = ffmpeg.input(overlay_image)
        
        if scale_mode == "contain":
            # ä¿æŒå®½é«˜æ¯”ï¼Œé€‚åº”è§†é¢‘å°ºå¯¸
            overlay_scaled = ffmpeg.filter(
                overlay_input,
                'scale',
                f'{video_width}:{video_height}:force_original_aspect_ratio=decrease'
            )
            # å±…ä¸­å åŠ 
            output_stream = ffmpeg.overlay(
                video_input,
                overlay_scaled,
                x='(W-w)/2',
                y='(H-h)/2'
            )
        else:  # stretch
            # æ‹‰ä¼¸åˆ°è§†é¢‘å°ºå¯¸
            overlay_scaled = ffmpeg.filter(
                overlay_input,
                'scale',
                f'{video_width}:{video_height}'
            )
            output_stream = ffmpeg.overlay(video_input, overlay_scaled)
        
        # è¾“å‡ºè§†é¢‘
        output_stream = ffmpeg.output(output_stream, output)
        ffmpeg.run(output_stream, overwrite_output=True, quiet=True)
    
    def merge_audio_video(
        self,
        video: str,
        audio: str,
        output: str,
        replace_audio: bool = True,
        audio_volume: float = 1.0
    ) -> str:
        """åˆå¹¶éŸ³é¢‘å’Œè§†é¢‘"""
        import ffmpeg
        
        video_input = ffmpeg.input(video)
        audio_input = ffmpeg.input(audio)
        
        if replace_audio:
            # æ›¿æ¢è§†é¢‘éŸ³é¢‘
            if audio_volume != 1.0:
                audio_filtered = ffmpeg.filter(audio_input, 'volume', audio_volume)
            else:
                audio_filtered = audio_input
            
            output_stream = ffmpeg.output(
                video_input,
                audio_filtered,
                output,
                vcodec='copy',  # å¤åˆ¶è§†é¢‘æµ
                acodec='aac',   # é‡æ–°ç¼–ç éŸ³é¢‘
                shortest=True   # ä»¥æœ€çŸ­æµä¸ºå‡†
            )
        else:
            # æ··åˆéŸ³é¢‘
            audio_filtered = ffmpeg.filter(audio_input, 'volume', audio_volume)
            mixed_audio = ffmpeg.filter([video_input['a'], audio_filtered], 'amix')
            
            output_stream = ffmpeg.output(
                video_input['v'],
                mixed_audio,
                output,
                vcodec='copy',
                acodec='aac'
            )
        
        ffmpeg.run(output_stream, overwrite_output=True, quiet=True)
        return output
    
    def create_video_from_image(
        self,
        image: str,
        audio: str,
        output: str,
        fps: int = 30
    ) -> str:
        """ä»Žå›¾åƒå’ŒéŸ³é¢‘åˆ›å»ºè§†é¢‘"""
        import ffmpeg
        
        # èŽ·å–éŸ³é¢‘æ—¶é•¿
        probe = ffmpeg.probe(audio)
        duration = float(probe['format']['duration'])
        
        # åˆ›å»ºè§†é¢‘
        image_input = ffmpeg.input(image, loop=1, t=duration, framerate=fps)
        audio_input = ffmpeg.input(audio)
        
        output_stream = ffmpeg.output(
            image_input,
            audio_input,
            output,
            vcodec='libx264',
            acodec='aac',
            pix_fmt='yuv420p',
            shortest=True
        )
        
        ffmpeg.run(output_stream, overwrite_output=True, quiet=True)
        return output
    
    def concat_videos(
        self,
        videos: List[str],
        output: str,
        bgm_path: str = None,
        bgm_volume: float = 0.2,
        bgm_mode: str = "loop"
    ) -> str:
        """æ‹¼æŽ¥å¤šä¸ªè§†é¢‘ç‰‡æ®µ"""
        import ffmpeg
        
        # åˆ›å»ºè¾“å…¥æµ
        inputs = [ffmpeg.input(video) for video in videos]
        
        # æ‹¼æŽ¥è§†é¢‘
        if len(inputs) == 1:
            concatenated = inputs[0]
        else:
            concatenated = ffmpeg.concat(*inputs, v=1, a=1)
        
        # æ·»åŠ èƒŒæ™¯éŸ³ä¹
        if bgm_path:
            bgm_input = ffmpeg.input(bgm_path)
            
            if bgm_mode == "loop":
                # å¾ªçŽ¯èƒŒæ™¯éŸ³ä¹
                bgm_looped = ffmpeg.filter(bgm_input, 'aloop', loop=-1, size=2**31-1)
            else:
                bgm_looped = bgm_input
            
            # è°ƒæ•´BGMéŸ³é‡
            bgm_adjusted = ffmpeg.filter(bgm_looped, 'volume', bgm_volume)
            
            # æ··åˆéŸ³é¢‘
            mixed_audio = ffmpeg.filter(
                [concatenated['a'], bgm_adjusted],
                'amix',
                inputs=2,
                duration='first'
            )
            
            output_stream = ffmpeg.output(
                concatenated['v'],
                mixed_audio,
                output,
                vcodec='libx264',
                acodec='aac'
            )
        else:
            # æ— èƒŒæ™¯éŸ³ä¹
            output_stream = ffmpeg.output(
                concatenated,
                output,
                vcodec='libx264',
                acodec='aac'
            )
        
        ffmpeg.run(output_stream, overwrite_output=True, quiet=True)
        return output
```

---

#### 7ï¸âƒ£ åŽæœŸåˆ¶ä½œé˜¶æ®µ (Post Production)
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `post_production()`

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def post_production(self, ctx: PipelineContext):
    """Step 7: Concatenate videos and add BGM."""
    self._report_progress(ctx.progress_callback, "concatenating", 0.85)
    
    storyboard = ctx.storyboard
    segment_paths = [frame.video_segment_path for frame in storyboard.frames]
    
    video_service = VideoService()
    
    final_video_path = video_service.concat_videos(
        videos=segment_paths,
        output=ctx.final_video_path,
        bgm_path=ctx.params.get("bgm_path"),
        bgm_volume=ctx.params.get("bgm_volume", 0.2),
        bgm_mode=ctx.params.get("bgm_mode", "loop")
    )
    
    storyboard.final_video_path = final_video_path
    storyboard.completed_at = datetime.now()
    
    # å¤åˆ¶åˆ°ç”¨æˆ·æŒ‡å®šè·¯å¾„ï¼ˆå¦‚æžœæä¾›ï¼‰
    user_specified_output = ctx.params.get("output_path")
    if user_specified_output:
        Path(user_specified_output).parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(final_video_path, user_specified_output)
        logger.info(f"ðŸ“¹ Final video copied to: {user_specified_output}")
        ctx.final_video_path = user_specified_output
        storyboard.final_video_path = user_specified_output
    
    logger.success(f"ðŸŽ¬ Video generation completed: {ctx.final_video_path}")
```

**BGMå¤„ç†è¯¦ç»†å®žçŽ°**:
```python
# åœ¨VideoService.concat_videos()ä¸­çš„BGMå¤„ç†é€»è¾‘
def _add_background_music(
    self,
    video_input,
    bgm_path: str,
    bgm_volume: float = 0.2,
    bgm_mode: str = "loop"
):
    """æ·»åŠ èƒŒæ™¯éŸ³ä¹åˆ°è§†é¢‘"""
    import ffmpeg
    
    # èŽ·å–è§†é¢‘æ—¶é•¿
    probe = ffmpeg.probe(video_input)
    video_duration = float(probe['format']['duration'])
    
    bgm_input = ffmpeg.input(bgm_path)
    
    if bgm_mode == "loop":
        # å¾ªçŽ¯æ¨¡å¼: BGMå¾ªçŽ¯æ’­æ”¾ç›´åˆ°è§†é¢‘ç»“æŸ
        bgm_looped = ffmpeg.filter(
            bgm_input,
            'aloop',
            loop=-1,  # æ— é™å¾ªçŽ¯
            size=2**31-1  # æœ€å¤§æ ·æœ¬æ•°
        )
        # æˆªå–åˆ°è§†é¢‘æ—¶é•¿
        bgm_trimmed = ffmpeg.filter(bgm_looped, 'atrim', duration=video_duration)
    elif bgm_mode == "fade":
        # æ·¡å…¥æ·¡å‡ºæ¨¡å¼
        bgm_faded = ffmpeg.filter(
            bgm_input,
            'afade',
            type='in',
            start_time=0,
            duration=2
        )
        bgm_faded = ffmpeg.filter(
            bgm_faded,
            'afade',
            type='out',
            start_time=video_duration-2,
            duration=2
        )
        bgm_trimmed = ffmpeg.filter(bgm_faded, 'atrim', duration=video_duration)
    else:  # once
        # å•æ¬¡æ’­æ”¾æ¨¡å¼
        bgm_trimmed = ffmpeg.filter(bgm_input, 'atrim', duration=video_duration)
    
    # è°ƒæ•´éŸ³é‡
    bgm_adjusted = ffmpeg.filter(bgm_trimmed, 'volume', bgm_volume)
    
    return bgm_adjusted
```

---

#### 8ï¸âƒ£ å®Œæˆé˜¶æ®µ (Finalize)
**æ–‡ä»¶**: `pixelle_video/pipelines/standard.py` - `finalize()`

**å®Œæ•´ä»£ç å®žçŽ°**:
```python
async def finalize(self, ctx: PipelineContext) -> VideoGenerationResult:
    """Step 8: Create result object and persist metadata."""
    self._report_progress(ctx.progress_callback, "completed", 1.0)
    
    video_path_obj = Path(ctx.final_video_path)
    file_size = video_path_obj.stat().st_size
    
    result = VideoGenerationResult(
        video_path=ctx.final_video_path,
        storyboard=ctx.storyboard,
        duration=ctx.storyboard.total_duration,
        file_size=file_size
    )
    
    ctx.result = result
    
    logger.info(f"âœ… Generated video: {ctx.final_video_path}")
    logger.info(f"   Duration: {ctx.storyboard.total_duration:.2f}s")
    logger.info(f"   Size: {file_size / (1024*1024):.2f} MB")
    logger.info(f"   Frames: {len(ctx.storyboard.frames)}")
    
    # æŒä¹…åŒ–å…ƒæ•°æ®
    await self._persist_task_data(ctx)
    
    return result

async def _persist_task_data(self, ctx: PipelineContext):
    """æŒä¹…åŒ–ä»»åŠ¡å…ƒæ•°æ®å’Œæ•…äº‹æ¿åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
    try:
        storyboard = ctx.storyboard
        result = ctx.result
        task_id = storyboard.config.task_id
        
        if not task_id:
            logger.warning("No task_id in storyboard, skipping persistence")
            return
        
        # æž„å»ºå…ƒæ•°æ®
        input_with_title = ctx.params.copy()
        input_with_title["text"] = ctx.input_text  # ç¡®ä¿åŒ…å«æ–‡æœ¬
        if not input_with_title.get("title"):
            input_with_title["title"] = storyboard.title
        
        metadata = {
            "task_id": task_id,
            "created_at": storyboard.created_at.isoformat() if storyboard.created_at else None,
            "completed_at": storyboard.completed_at.isoformat() if storyboard.completed_at else None,
            "status": "completed",
            
            "input": input_with_title,
            
            "result": {
                "video_path": result.video_path,
                "duration": result.duration,
                "file_size": result.file_size,
                "n_frames": len(storyboard.frames)
            },
            
            "config": {
                "llm_model": self.core.config.get("llm", {}).get("model", "unknown"),
                "llm_base_url": self.core.config.get("llm", {}).get("base_url", "unknown"),
                "comfyui_url": self.core.config.get("comfyui", {}).get("comfyui_url", "unknown"),
                "runninghub_enabled": bool(self.core.config.get("comfyui", {}).get("runninghub_api_key")),
            }
        }
        
        # ä¿å­˜å…ƒæ•°æ®
        await self.core.persistence.save_task_metadata(task_id, metadata)
        logger.info(f"ðŸ’¾ Saved task metadata: {task_id}")
        
        # ä¿å­˜æ•…äº‹æ¿
        await self.core.persistence.save_storyboard(task_id, storyboard)
        logger.info(f"ðŸ’¾ Saved storyboard: {task_id}")
        
    except Exception as e:
        logger.error(f"Failed to persist task data: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ - æŒä¹…åŒ–å¤±è´¥ä¸åº”è¯¥ç ´åè§†é¢‘ç”Ÿæˆ
```

**æ•°æ®æ¨¡åž‹å®šä¹‰**:
```python
# pixelle_video/models/storyboard.py
@dataclass
class VideoGenerationResult:
    """è§†é¢‘ç”Ÿæˆç»“æžœ"""
    video_path: str
    storyboard: 'Storyboard'
    duration: float
    file_size: int
    
    @property
    def size_mb(self) -> float:
        """æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
        return self.file_size / (1024 * 1024)
    
    @property
    def frames_count(self) -> int:
        """å¸§æ•°"""
        return len(self.storyboard.frames)
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "video_path": self.video_path,
            "duration": self.duration,
            "file_size": self.file_size,
            "size_mb": self.size_mb,
            "frames_count": self.frames_count,
            "title": self.storyboard.title,
            "created_at": self.storyboard.created_at.isoformat() if self.storyboard.created_at else None,
            "completed_at": self.storyboard.completed_at.isoformat() if self.storyboard.completed_at else None
        }

# æŒä¹…åŒ–æœåŠ¡å®žçŽ°
class PersistenceService:
    async def save_task_metadata(self, task_id: str, metadata: dict):
        """ä¿å­˜ä»»åŠ¡å…ƒæ•°æ®"""
        metadata_path = Path(f"output/{task_id}/metadata.json")
        metadata_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    async def save_storyboard(self, task_id: str, storyboard: Storyboard):
        """ä¿å­˜æ•…äº‹æ¿"""
        storyboard_path = Path(f"output/{task_id}/storyboard.json")
        
        # åºåˆ—åŒ–æ•…äº‹æ¿
        storyboard_data = {
            "title": storyboard.title,
            "total_duration": storyboard.total_duration,
            "created_at": storyboard.created_at.isoformat() if storyboard.created_at else None,
            "completed_at": storyboard.completed_at.isoformat() if storyboard.completed_at else None,
            "config": asdict(storyboard.config),
            "frames": [asdict(frame) for frame in storyboard.frames],
            "content_metadata": storyboard.content_metadata
        }
        
        with open(storyboard_path, 'w', encoding='utf-8') as f:
            json.dump(storyboard_data, f, indent=2, ensure_ascii=False)
    
    async def load_task_metadata(self, task_id: str) -> dict:
        """åŠ è½½ä»»åŠ¡å…ƒæ•°æ®"""
        metadata_path = Path(f"output/{task_id}/metadata.json")
        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata not found for task {task_id}")
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            return json.load(f)
```

---

## ðŸ› ï¸ æŠ€æœ¯æ ˆè¯¦è§£

### AIæ¨¡åž‹æ”¯æŒ

#### å¤§è¯­è¨€æ¨¡åž‹ (LLM)
**æ–‡ä»¶**: `pixelle_video/llm_presets.py`
- **GPTç³»åˆ—**: GPT-4o, GPT-4o-mini
- **å›½äº§æ¨¡åž‹**: é€šä¹‰åƒé—®, DeepSeek, æ™ºè°±GLM
- **å¼€æºæ¨¡åž‹**: Ollama (æœ¬åœ°éƒ¨ç½²)

#### å›¾åƒç”Ÿæˆæ¨¡åž‹
**å·¥ä½œæµ**: `workflows/*/image_*.json`
- **FLUX**: é«˜è´¨é‡å›¾åƒç”Ÿæˆ
- **SDXL**: Stable Diffusion XL
- **é€šä¹‰ä¸‡ç›¸**: é˜¿é‡Œäº‘å›¾åƒç”Ÿæˆ
- **Qwen**: é€šä¹‰åƒé—®å›¾åƒæ¨¡åž‹

#### è§†é¢‘ç”Ÿæˆæ¨¡åž‹  
**å·¥ä½œæµ**: `workflows/*/video_*.json`
- **WAN 2.1**: æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ
- **WAN 2.2**: å‡çº§ç‰ˆè§†é¢‘ç”Ÿæˆ
- **FusionX**: èžåˆå¢žå¼ºæ¨¡åž‹

#### è¯­éŸ³åˆæˆ (TTS)
**å·¥ä½œæµ**: `workflows/*/tts_*.json`
- **Edge-TTS**: å¾®è½¯å…è´¹TTS
- **Index-TTS**: æ”¯æŒå£°éŸ³å…‹éš†
- **Spark TTS**: è®¯é£žè¯­éŸ³åˆæˆ

### æ ¸å¿ƒæœåŠ¡

#### LLMæœåŠ¡ (`services/llm_service.py`)
- ç»Ÿä¸€çš„LLMè°ƒç”¨æŽ¥å£
- æ”¯æŒå¤šç§APIæ ¼å¼
- è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†

#### TTSæœåŠ¡ (`services/tts_service.py`)
- æœ¬åœ°TTSå’ŒComfyUIå·¥ä½œæµ
- å£°éŸ³å…‹éš†æ”¯æŒ
- éŸ³é¢‘æ ¼å¼è½¬æ¢

#### åª’ä½“æœåŠ¡ (`services/media.py`)
- å›¾åƒå’Œè§†é¢‘ç”Ÿæˆ
- ComfyUIå·¥ä½œæµæ‰§è¡Œ
- RunningHubäº‘ç«¯è°ƒç”¨

#### è§†é¢‘æœåŠ¡ (`services/video.py`)
- FFmpegè§†é¢‘å¤„ç†
- éŸ³è§†é¢‘åˆæˆ
- æ ¼å¼è½¬æ¢å’ŒåŽ‹ç¼©

---

## âš™ï¸ å·¥ä½œæµé…ç½®

### ç›®å½•ç»“æž„
```
workflows/
â”œâ”€â”€ selfhost/          # æœ¬åœ°éƒ¨ç½²å·¥ä½œæµ
â”‚   â”œâ”€â”€ image_flux.json      # FLUXå›¾åƒç”Ÿæˆ
â”‚   â”œâ”€â”€ video_wan2.1.json    # WAN 2.1è§†é¢‘ç”Ÿæˆ
â”‚   â””â”€â”€ tts_edge.json        # Edge-TTSè¯­éŸ³åˆæˆ
â””â”€â”€ runninghub/        # äº‘ç«¯éƒ¨ç½²å·¥ä½œæµ
    â”œâ”€â”€ image_flux.json      # äº‘ç«¯FLUX
    â”œâ”€â”€ video_wan2.2.json    # äº‘ç«¯WAN 2.2
    â””â”€â”€ tts_spark.json       # äº‘ç«¯è®¯é£žTTS
```

### å·¥ä½œæµæ ¼å¼
åŸºäºŽComfyUIçš„JSONé…ç½®æ–‡ä»¶:

```json
{
  "èŠ‚ç‚¹ID": {
    "inputs": {
      "å‚æ•°å": "å‚æ•°å€¼",
      "è¿žæŽ¥": ["æºèŠ‚ç‚¹ID", è¾“å‡ºç´¢å¼•]
    },
    "class_type": "èŠ‚ç‚¹ç±»åž‹",
    "_meta": {"title": "èŠ‚ç‚¹æ ‡é¢˜"}
  }
}
```

### å‚æ•°æ›¿æ¢
å·¥ä½œæµæ”¯æŒåŠ¨æ€å‚æ•°æ›¿æ¢:
- `$prompt.value`: å›¾åƒæç¤ºè¯
- `$width.value`: å›¾åƒå®½åº¦  
- `$height.value`: å›¾åƒé«˜åº¦
- `$duration.value`: è§†é¢‘æ—¶é•¿

---

## ðŸš€ éƒ¨ç½²æ–¹æ¡ˆ

### æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | æˆæœ¬ | é€Ÿåº¦ | è´¨é‡ | æŠ€æœ¯è¦æ±‚ |
|------|------|------|------|----------|
| å®Œå…¨å…è´¹ | 0å…ƒ | æ…¢ | ä¸­ç­‰ | é«˜ |
| æŽ¨èæ–¹æ¡ˆ | æžä½Ž | å¿« | é«˜ | ä¸­ç­‰ |
| äº‘ç«¯æ–¹æ¡ˆ | è¾ƒé«˜ | æœ€å¿« | æœ€é«˜ | ä½Ž |

### 1. å®Œå…¨å…è´¹æ–¹æ¡ˆ
**é…ç½®**: Ollama (æœ¬åœ°LLM) + ComfyUI (æœ¬åœ°éƒ¨ç½²)
- **LLM**: Ollamaè¿è¡ŒQwenç­‰å¼€æºæ¨¡åž‹
- **å›¾åƒ**: æœ¬åœ°ComfyUI + FLUX/SDXL
- **TTS**: Edge-TTS (å…è´¹)
- **æˆæœ¬**: 0å…ƒ
- **è¦æ±‚**: æ˜¾å¡8GB+ VRAM

### 2. æŽ¨èæ–¹æ¡ˆ â­
**é…ç½®**: é€šä¹‰åƒé—® (äº‘ç«¯LLM) + ComfyUI (æœ¬åœ°éƒ¨ç½²)
- **LLM**: é€šä¹‰åƒé—®API (æžä½Žæˆæœ¬)
- **å›¾åƒ**: æœ¬åœ°ComfyUI + FLUX
- **TTS**: Edge-TTSæˆ–æœ¬åœ°Index-TTS
- **æˆæœ¬**: æ¯ä¸ªè§†é¢‘çº¦0.1-0.5å…ƒ
- **è¦æ±‚**: æ˜¾å¡4GB+ VRAM

### 3. äº‘ç«¯æ–¹æ¡ˆ
**é…ç½®**: OpenAI (äº‘ç«¯LLM) + RunningHub (äº‘ç«¯åª’ä½“)
- **LLM**: GPT-4o API
- **å›¾åƒ**: RunningHub FLUX/SDXL
- **TTS**: RunningHub Spark TTS
- **æˆæœ¬**: æ¯ä¸ªè§†é¢‘çº¦2-5å…ƒ
- **è¦æ±‚**: ä»…éœ€ç½‘ç»œè¿žæŽ¥

---

## ðŸ’¡ ä½¿ç”¨å»ºè®®

### æ–°æ‰‹å…¥é—¨
1. **ä½¿ç”¨Windowsä¸€é”®åŒ…**: å…å®‰è£…ï¼Œå¼€ç®±å³ç”¨
2. **é€‰æ‹©æŽ¨èæ–¹æ¡ˆ**: é€šä¹‰åƒé—® + æœ¬åœ°ComfyUI
3. **ä»Žé»˜è®¤æ¨¡æ¿å¼€å§‹**: ä½¿ç”¨å†…ç½®æ¨¡æ¿å’Œå·¥ä½œæµ
4. **é€æ­¥è‡ªå®šä¹‰**: ç†Ÿæ‚‰åŽå†è°ƒæ•´å‚æ•°

### è¿›é˜¶ç”¨æˆ·
1. **æœ¬åœ°éƒ¨ç½²ComfyUI**: æ›´å¥½çš„æŽ§åˆ¶å’Œè‡ªå®šä¹‰
2. **è‡ªå®šä¹‰å·¥ä½œæµ**: æ ¹æ®éœ€æ±‚è°ƒæ•´AIæ¨¡åž‹
3. **æ‰¹é‡ç”Ÿæˆ**: åˆ©ç”¨å¹¶è¡Œå¤„ç†æå‡æ•ˆçŽ‡
4. **æ¨¡æ¿å®šåˆ¶**: åˆ›å»ºä¸“å±žè§†é¢‘é£Žæ ¼

### ä¸“ä¸šç”¨æˆ·
1. **æ··åˆéƒ¨ç½²**: å…³é”®æ­¥éª¤æœ¬åœ°ï¼Œè¾…åŠ©æ­¥éª¤äº‘ç«¯
2. **å·¥ä½œæµä¼˜åŒ–**: æ·±åº¦å®šåˆ¶ComfyUIèŠ‚ç‚¹
3. **APIé›†æˆ**: é›†æˆåˆ°çŽ°æœ‰å·¥ä½œæµ
4. **æ€§èƒ½è°ƒä¼˜**: ä¼˜åŒ–å¹¶å‘å’Œç¼“å­˜ç­–ç•¥

### æˆæœ¬ä¼˜åŒ–å»ºè®®
1. **é™æ€æ¨¡æ¿**: çº¯æ–‡å­—è§†é¢‘ï¼Œè·³è¿‡åª’ä½“ç”Ÿæˆ
2. **æ‰¹é‡å¤„ç†**: ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘åˆ†æ‘Šæˆæœ¬
3. **æœ¬åœ°ç¼“å­˜**: å¤ç”¨ç›¸ä¼¼çš„å›¾åƒå’ŒéŸ³é¢‘
4. **å‚æ•°è°ƒä¼˜**: å¹³è¡¡è´¨é‡å’Œæˆæœ¬

---

## ðŸ”§ æ•…éšœæŽ’é™¤

### å¸¸è§é—®é¢˜

#### 1. çŽ¯å¢ƒé—®é¢˜
- **Pythonç‰ˆæœ¬**: éœ€è¦3.11+
- **ä¾èµ–å®‰è£…**: ä½¿ç”¨`uv`ç®¡ç†ä¾èµ–
- **FFmpeg**: è§†é¢‘å¤„ç†å¿…éœ€

#### 2. ComfyUIè¿žæŽ¥
- **æ£€æŸ¥æœåŠ¡**: `http://127.0.0.1:8188`
- **å·¥ä½œæµå…¼å®¹**: ç¡®ä¿èŠ‚ç‚¹ç‰ˆæœ¬åŒ¹é…
- **æ˜¾å­˜ä¸è¶³**: é™ä½Žåˆ†è¾¨çŽ‡æˆ–æ‰¹æ¬¡å¤§å°

#### 3. APIé…ç½®
- **å¯†é’¥æœ‰æ•ˆæ€§**: æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®
- **ç½‘ç»œè¿žæŽ¥**: ç¡®ä¿èƒ½è®¿é—®APIæœåŠ¡
- **é…é¢é™åˆ¶**: æ£€æŸ¥APIä½¿ç”¨é¢åº¦

#### 4. ç”Ÿæˆè´¨é‡
- **æç¤ºè¯ä¼˜åŒ–**: è°ƒæ•´prompt_prefix
- **æ¨¡åž‹é€‰æ‹©**: å°è¯•ä¸åŒçš„AIæ¨¡åž‹
- **å‚æ•°è°ƒæ•´**: ä¿®æ”¹stepsã€cfgç­‰å‚æ•°

---

## ðŸ“š æ‰©å±•å¼€å‘

### è‡ªå®šä¹‰ç®¡é“
ç»§æ‰¿`LinearVideoPipeline`åˆ›å»ºè‡ªå®šä¹‰æµç¨‹:

```python
# custom_pipeline.py
from pixelle_video.pipelines.linear import LinearVideoPipeline, PipelineContext
from pixelle_video.models.storyboard import VideoGenerationResult

class CustomPipeline(LinearVideoPipeline):
    """è‡ªå®šä¹‰è§†é¢‘ç”Ÿæˆç®¡é“"""
    
    async def plan_visuals(self, ctx: PipelineContext):
        """è‡ªå®šä¹‰è§†è§‰è§„åˆ’é€»è¾‘"""
        # ä¾‹: ä½¿ç”¨å›ºå®šçš„å›¾åƒé£Žæ ¼
        ctx.image_prompts = []
        for narration in ctx.narrations:
            # è‡ªå®šä¹‰æç¤ºè¯ç”Ÿæˆé€»è¾‘
            custom_prompt = f"anime style, {narration}, high quality, detailed"
            ctx.image_prompts.append(custom_prompt)
        
        logger.info(f"âœ… Generated {len(ctx.image_prompts)} custom prompts")
    
    async def produce_assets(self, ctx: PipelineContext):
        """è‡ªå®šä¹‰èµ„äº§ç”Ÿäº§æµç¨‹"""
        # ä¾‹: æ·»åŠ è‡ªå®šä¹‰å¤„ç†æ­¥éª¤
        await super().produce_assets(ctx)
        
        # åŽå¤„ç†: ä¸ºæ¯ä¸ªå¸§æ·»åŠ æ°´å°
        for frame in ctx.storyboard.frames:
            if frame.composed_image_path:
                await self._add_watermark(frame.composed_image_path)
    
    async def _add_watermark(self, image_path: str):
        """æ·»åŠ æ°´å°"""
        from PIL import Image, ImageDraw, ImageFont
        
        with Image.open(image_path) as img:
            draw = ImageDraw.Draw(img)
            
            # æ·»åŠ æ°´å°æ–‡å­—
            font = ImageFont.load_default()
            watermark_text = "Generated by Pixelle-Video"
            
            # è®¡ç®—ä½ç½®ï¼ˆå³ä¸‹è§’ï¼‰
            bbox = draw.textbbox((0, 0), watermark_text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            x = img.width - text_width - 10
            y = img.height - text_height - 10
            
            # ç»˜åˆ¶æ°´å°
            draw.text((x, y), watermark_text, fill=(255, 255, 255, 128), font=font)
            
            # ä¿å­˜
            img.save(image_path)

# ä½¿ç”¨è‡ªå®šä¹‰ç®¡é“
async def generate_custom_video(topic: str):
    from pixelle_video.service import PixelleVideoCore
    
    core = PixelleVideoCore()
    pipeline = CustomPipeline(core)
    
    result = await pipeline(
        text=topic,
        mode="generate",
        n_scenes=5,
        frame_template="1080x1920/anime.html"  # è‡ªå®šä¹‰æ¨¡æ¿
    )
    
    return result
```

### è‡ªå®šä¹‰æœåŠ¡
å®žçŽ°æ–°çš„AIæœåŠ¡:

```python
# custom_tts_service.py
from pixelle_video.services.tts_service import TTSService

class CustomTTSService(TTSService):
    """è‡ªå®šä¹‰TTSæœåŠ¡"""
    
    async def _generate_custom_tts(
        self,
        text: str,
        voice: str = "custom_voice",
        output_path: str = None,
        **kwargs
    ) -> str:
        """è‡ªå®šä¹‰TTSå®žçŽ°"""
        
        # ä¾‹: è°ƒç”¨ç¬¬ä¸‰æ–¹TTS API
        import httpx
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.custom-tts.com/synthesize",
                json={
                    "text": text,
                    "voice": voice,
                    "format": "mp3"
                },
                headers={"Authorization": f"Bearer {self.api_key}"}
            )
            
            response.raise_for_status()
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            with open(output_path, 'wb') as f:
                f.write(response.content)
            
            return output_path
    
    async def __call__(self, **kwargs):
        """é‡å†™è°ƒç”¨æ–¹æ³•ä»¥æ”¯æŒè‡ªå®šä¹‰TTS"""
        inference_mode = kwargs.get("inference_mode", "local")
        
        if inference_mode == "custom":
            return await self._generate_custom_tts(**kwargs)
        else:
            return await super().__call__(**kwargs)

# æ³¨å†Œè‡ªå®šä¹‰æœåŠ¡
from pixelle_video.service import PixelleVideoCore

class CustomPixelleVideoCore(PixelleVideoCore):
    def __init__(self, config_path: str = None):
        super().__init__(config_path)
        # æ›¿æ¢TTSæœåŠ¡
        self.tts = CustomTTSService(self.config)
```

### è‡ªå®šä¹‰æ¨¡æ¿
åˆ›å»ºHTMLæ¨¡æ¿:

```html
<!-- templates/custom/my_template.html -->
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <style>
        body {
            margin: 0;
            padding: 0;
            width: 1080px;
            height: 1920px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            font-family: 'Arial', sans-serif;
            color: white;
        }
        
        .title {
            font-size: 48px;
            font-weight: bold;
            text-align: center;
            margin-bottom: 40px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);
        }
        
        .content-area {
            width: 90%;
            height: 60%;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }
        
        .media-container {
            width: 100%;
            height: 70%;
            border-radius: 20px;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            margin-bottom: 30px;
        }
        
        .media-container img,
        .media-container video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        
        .text-overlay {
            background: rgba(0,0,0,0.7);
            padding: 20px 30px;
            border-radius: 15px;
            font-size: 32px;
            text-align: center;
            line-height: 1.4;
            max-width: 90%;
            backdrop-filter: blur(10px);
        }
        
        .watermark {
            position: absolute;
            bottom: 20px;
            right: 20px;
            font-size: 16px;
            opacity: 0.7;
        }
    </style>
</head>
<body>
    <div class="title">{{title}}</div>
    
    <div class="content-area">
        <div class="media-container">
            {% if image %}
                <img src="{{image}}" alt="Generated content">
            {% else %}
                <div style="background: rgba(255,255,255,0.1); display: flex; align-items: center; justify-content: center; font-size: 24px;">
                    No Media
                </div>
            {% endif %}
        </div>
        
        <div class="text-overlay">
            {{text}}
        </div>
    </div>
    
    <div class="watermark">
        Frame {{ext.index}} | Pixelle-Video
    </div>
</body>
</html>
```

### è‡ªå®šä¹‰å·¥ä½œæµ
åˆ›å»ºComfyUIå·¥ä½œæµ:

```json
{
  "workflow_name": "custom_image_generation",
  "description": "è‡ªå®šä¹‰å›¾åƒç”Ÿæˆå·¥ä½œæµ",
  "nodes": {
    "1": {
      "inputs": {
        "text": "$prompt.value",
        "width": "$width.value",
        "height": "$height.value"
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "Prompt Input"
      }
    },
    "2": {
      "inputs": {
        "seed": 42,
        "steps": 20,
        "cfg": 7.5,
        "sampler_name": "euler",
        "scheduler": "normal",
        "denoise": 1,
        "model": ["3", 0],
        "positive": ["1", 0],
        "negative": ["4", 0],
        "latent_image": ["5", 0]
      },
      "class_type": "KSampler",
      "_meta": {
        "title": "Sampler"
      }
    },
    "3": {
      "inputs": {
        "ckpt_name": "sd_xl_base_1.0.safetensors"
      },
      "class_type": "CheckpointLoaderSimple",
      "_meta": {
        "title": "Load Checkpoint"
      }
    },
    "4": {
      "inputs": {
        "text": "low quality, blurry, distorted",
        "clip": ["3", 1]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "Negative Prompt"
      }
    },
    "5": {
      "inputs": {
        "width": "$width.value",
        "height": "$height.value",
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": {
        "title": "Empty Latent"
      }
    },
    "6": {
      "inputs": {
        "samples": ["2", 0],
        "vae": ["3", 2]
      },
      "class_type": "VAEDecode",
      "_meta": {
        "title": "VAE Decode"
      }
    },
    "7": {
      "inputs": {
        "filename_prefix": "custom_output",
        "images": ["6", 0]
      },
      "class_type": "SaveImage",
      "_meta": {
        "title": "Save Image"
      }
    }
  }
}
```

### å®Œæ•´ç¤ºä¾‹: è‡ªå®šä¹‰è§†é¢‘ç”Ÿæˆå™¨

```python
# complete_custom_example.py
import asyncio
from pathlib import Path
from pixelle_video.service import PixelleVideoCore
from pixelle_video.pipelines.linear import LinearVideoPipeline, PipelineContext
from pixelle_video.models.storyboard import VideoGenerationResult

class MovieTrailerPipeline(LinearVideoPipeline):
    """ç”µå½±é¢„å‘Šç‰‡é£Žæ ¼çš„è§†é¢‘ç”Ÿæˆç®¡é“"""
    
    async def generate_content(self, ctx: PipelineContext):
        """ç”Ÿæˆç”µå½±é¢„å‘Šç‰‡é£Žæ ¼çš„åˆ†é•œ"""
        topic = ctx.input_text
        
        # é¢„å‘Šç‰‡å›ºå®šç»“æž„
        trailer_structure = [
            f"åœ¨ä¸€ä¸ªå……æ»¡{topic}çš„ä¸–ç•Œé‡Œ...",
            f"ä¸€ä¸ªå…³äºŽ{topic}çš„æ•…äº‹å³å°†å±•å¼€",
            f"å½“{topic}é‡åˆ°å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜",
            f"è‹±é›„å¿…é¡»é¢å¯¹{topic}çš„è€ƒéªŒ",
            f"è¿™å°†æ˜¯ä¸€åœºå…³äºŽ{topic}çš„å²è¯—å†’é™©"
        ]
        
        ctx.narrations = trailer_structure
        logger.info(f"âœ… Generated movie trailer structure with {len(ctx.narrations)} scenes")
    
    async def plan_visuals(self, ctx: PipelineContext):
        """ç”Ÿæˆç”µå½±é£Žæ ¼çš„è§†è§‰æç¤ºè¯"""
        cinematic_prompts = []
        
        for i, narration in enumerate(ctx.narrations):
            if i == 0:
                # å¼€åœº: å®½é˜”çš„æ™¯è§‚é•œå¤´
                prompt = f"cinematic wide shot, epic landscape, {narration}, dramatic lighting, film grain"
            elif i == len(ctx.narrations) - 1:
                # ç»“å°¾: åŠ¨ä½œé•œå¤´
                prompt = f"dynamic action shot, {narration}, intense lighting, motion blur, cinematic"
            else:
                # ä¸­é—´: è§’è‰²ç‰¹å†™
                prompt = f"cinematic close-up, character portrait, {narration}, dramatic shadows, film noir style"
            
            cinematic_prompts.append(prompt)
        
        ctx.image_prompts = cinematic_prompts
        logger.info(f"âœ… Generated {len(ctx.image_prompts)} cinematic prompts")

async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆå§‹åŒ–æ ¸å¿ƒæœåŠ¡
    core = PixelleVideoCore()
    
    # åˆ›å»ºè‡ªå®šä¹‰ç®¡é“
    pipeline = MovieTrailerPipeline(core)
    
    # ç”Ÿæˆè§†é¢‘
    result = await pipeline(
        text="äººå·¥æ™ºèƒ½",
        mode="generate",
        frame_template="1920x1080/movie_trailer.html",
        media_workflow="image_flux.json",
        tts_inference_mode="local",
        voice_id="zh-CN-YunjianNeural",
        tts_speed=1.0,
        bgm_path="bgm/epic_trailer.mp3",
        bgm_volume=0.3
    )
    
    print(f"ðŸŽ¬ è§†é¢‘ç”Ÿæˆå®Œæˆ!")
    print(f"   è·¯å¾„: {result.video_path}")
    print(f"   æ—¶é•¿: {result.duration:.2f}ç§’")
    print(f"   å¤§å°: {result.size_mb:.2f}MB")
    print(f"   å¸§æ•°: {result.frames_count}")

if __name__ == "__main__":
    asyncio.run(main())
```

### APIè°ƒç”¨ç¤ºä¾‹

```python
# api_usage_example.py
import asyncio
from pixelle_video.service import PixelleVideoCore

async def simple_video_generation():
    """ç®€å•çš„è§†é¢‘ç”Ÿæˆç¤ºä¾‹"""
    
    # åˆå§‹åŒ–æœåŠ¡
    core = PixelleVideoCore()
    
    # ä½¿ç”¨æ ‡å‡†ç®¡é“ç”Ÿæˆè§†é¢‘
    from pixelle_video.pipelines.standard import StandardPipeline
    pipeline = StandardPipeline(core)
    
    # å®šä¹‰è¿›åº¦å›žè°ƒ
    def progress_callback(event):
        print(f"Progress: {event.progress:.1%} - {event.event_type}")
        if hasattr(event, 'frame_current') and event.frame_current:
            print(f"  Frame {event.frame_current}/{event.frame_total}")
    
    # ç”Ÿæˆè§†é¢‘
    result = await pipeline(
        text="å¦‚ä½•å­¦ä¹ äººå·¥æ™ºèƒ½",
        mode="generate",
        n_scenes=3,
        frame_template="1080x1920/default.html",
        tts_inference_mode="local",
        voice_id="zh-CN-YunjianNeural",
        progress_callback=progress_callback
    )
    
    return result

async def batch_video_generation():
    """æ‰¹é‡è§†é¢‘ç”Ÿæˆç¤ºä¾‹"""
    
    topics = [
        "äººå·¥æ™ºèƒ½çš„å‘å±•åŽ†ç¨‹",
        "æœºå™¨å­¦ä¹ åŸºç¡€çŸ¥è¯†", 
        "æ·±åº¦å­¦ä¹ åº”ç”¨æ¡ˆä¾‹"
    ]
    
    core = PixelleVideoCore()
    pipeline = StandardPipeline(core)
    
    results = []
    for i, topic in enumerate(topics):
        print(f"Generating video {i+1}/{len(topics)}: {topic}")
        
        result = await pipeline(
            text=topic,
            mode="generate",
            n_scenes=4,
            output_path=f"output/batch_video_{i+1}.mp4"
        )
        
        results.append(result)
        print(f"âœ… Video {i+1} completed: {result.video_path}")
    
    return results

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    # å•ä¸ªè§†é¢‘ç”Ÿæˆ
    result = asyncio.run(simple_video_generation())
    print(f"Generated: {result.video_path}")
    
    # æ‰¹é‡è§†é¢‘ç”Ÿæˆ
    # results = asyncio.run(batch_video_generation())
    # print(f"Generated {len(results)} videos")
```

---

## ðŸ“– å‚è€ƒèµ„æº

- **é¡¹ç›®ä¸»é¡µ**: https://github.com/AIDC-AI/Pixelle-Video
- **ä½¿ç”¨æ–‡æ¡£**: https://aidc-ai.github.io/Pixelle-Video/zh
- **ComfyUI**: https://github.com/comfyanonymous/ComfyUI
- **è§†é¢‘æ•™ç¨‹**: https://www.bilibili.com/video/BV1WzyGBnEVp

---

*æœ¬æ–‡æ¡£åŸºäºŽPixelle-Video v0.1.11ç‰ˆæœ¬ç¼–å†™ï¼Œå¦‚æœ‰æ›´æ–°è¯·å‚è€ƒæœ€æ–°ç‰ˆæœ¬ã€‚*